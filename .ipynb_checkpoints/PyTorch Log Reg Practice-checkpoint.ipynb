{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 2.1609\n",
      "Epoch [1/5], Step [200/600], Loss: 2.0933\n",
      "Epoch [1/5], Step [300/600], Loss: 2.0281\n",
      "Epoch [1/5], Step [400/600], Loss: 1.9155\n",
      "Epoch [1/5], Step [500/600], Loss: 1.8742\n",
      "Epoch [1/5], Step [600/600], Loss: 1.7646\n",
      "Epoch [2/5], Step [100/600], Loss: 1.7386\n",
      "Epoch [2/5], Step [200/600], Loss: 1.7198\n",
      "Epoch [2/5], Step [300/600], Loss: 1.6182\n",
      "Epoch [2/5], Step [400/600], Loss: 1.5430\n",
      "Epoch [2/5], Step [500/600], Loss: 1.4303\n",
      "Epoch [2/5], Step [600/600], Loss: 1.4198\n",
      "Epoch [3/5], Step [100/600], Loss: 1.4585\n",
      "Epoch [3/5], Step [200/600], Loss: 1.4025\n",
      "Epoch [3/5], Step [300/600], Loss: 1.4033\n",
      "Epoch [3/5], Step [400/600], Loss: 1.2876\n",
      "Epoch [3/5], Step [500/600], Loss: 1.3390\n",
      "Epoch [3/5], Step [600/600], Loss: 1.1907\n",
      "Epoch [4/5], Step [100/600], Loss: 1.1660\n",
      "Epoch [4/5], Step [200/600], Loss: 1.1919\n",
      "Epoch [4/5], Step [300/600], Loss: 1.1988\n",
      "Epoch [4/5], Step [400/600], Loss: 1.1465\n",
      "Epoch [4/5], Step [500/600], Loss: 1.1589\n",
      "Epoch [4/5], Step [600/600], Loss: 1.0211\n",
      "Epoch [5/5], Step [100/600], Loss: 0.9396\n",
      "Epoch [5/5], Step [200/600], Loss: 1.0261\n",
      "Epoch [5/5], Step [300/600], Loss: 1.1216\n",
      "Epoch [5/5], Step [400/600], Loss: 0.9498\n",
      "Epoch [5/5], Step [500/600], Loss: 0.9942\n",
      "Epoch [5/5], Step [600/600], Loss: 0.9737\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "input_size=784\n",
    "num_classes=10\n",
    "num_epochs=5\n",
    "batch_size=100\n",
    "learning_rate=0.001\n",
    "\n",
    "#data\n",
    "train_dataset=torchvision.datasets.MNIST(root=\"../../data\",train=True,transform=transforms.ToTensor(),download=True)\n",
    "test_dataset=torchvision.datasets.MNIST(root=\"../../data\",train=False,transform=transforms.ToTensor())\n",
    "\n",
    "#batch\n",
    "train_loader=torch.utils.data.DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True)\n",
    "test_loader=torch.utils.data.DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=False)\n",
    "\n",
    "#modeling(logistic인데 linear로 부르는군..)\n",
    "model=nn.Linear(input_size,num_classes)\n",
    "\n",
    "#softmax알아서 해주네. 갓갓...\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "\n",
    "#training\n",
    "total_step=len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images,labels) in enumerate(train_loader):\n",
    "        images=images.reshape(-1,28*28)\n",
    "        \n",
    "        outputs=model(images)\n",
    "        loss=criterion(outputs,labels)\n",
    "        \n",
    "        #다시 나온 그 이상한놈들. training을 해주는거같다.\n",
    "        #backprogpagation을 단계화시킨 느낌\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if(i+1)%100==0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                    .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the 10000 test images: 82 %\n"
     ]
    }
   ],
   "source": [
    "#새 세션을 열었다.\n",
    "with torch.no_grad():\n",
    "    correct=0\n",
    "    total=0\n",
    "    for images, labels in test_loader:\n",
    "        images=images.reshape(-1,28*28)\n",
    "        outputs=model(images)\n",
    "        #튜플로 나오나본데 먼지 궁금하넹\n",
    "        #뒤에 붙은 1은 dimension임! (그 dim을 기준으로 나머지 애들을 싹다 비교해서 max를 내뱉음)\n",
    "        #아하 matrix에서 비교하면 첫번째껀 각 행에서의 max값이, 두번째껀 그 index가 return되네\n",
    "        _,predicted=torch.max(outputs.data,1)\n",
    "        total+=labels.size(0)\n",
    "        #원소 type이 boolean인 tensor matrix에서의 전체sum\n",
    "        correct+=(predicted==labels).sum()\n",
    "        \n",
    "    print('Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
